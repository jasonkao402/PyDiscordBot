import json
import re
import tqdm
import jieba
import numpy as np
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import CountVectorizer

hist_path = "subsystem_tests/chat_history/è¯å±±è«–åŠ.json"
# hist_path = "subsystem_tests/chat_history/deepseek.txt"
# model_name = "distiluse-base-multilingual-cased-v1"
embedder = "paraphrase-multilingual-MiniLM-L12-v2"

embedder = SentenceTransformer(embedder, device="cuda:0")
kw_model = KeyBERT(model=embedder)

ban_msgs_regex = [
    r"https?://[^\s]+",  # Match https links
    r"(<a?)?:\w+:(\d{18}>)?"  # Match Discord emojis
    # "\\d{4}-\\d{2}-\\d{2}", # Match dates
]

def filter_msg(msg):
    # Filter messages (raw messages)
    if msg == "":
        return msg
    msg = re.sub(r"\n+", " ", msg)  # Replace newlines with spaces
    for regex in ban_msgs_regex:
        msg = re.sub(regex, "", msg)
    msg = re.sub(r" +", " ", msg)  # Replace multiple spaces with a single space
    return msg.strip()

# Read JSON file
with open(hist_path, 'r', encoding='utf-8') as f:
    data = json.load(f)

authors = set()
messages = []
for i in data['messages']:
    content = i['content']
    author = int(i['author']['id'])
    content = filter_msg(content)
    authors.add(author)
    if content == "":
        continue
    messages.append(content)
# messages = []
# with open(hist_path, 'r', encoding='utf-8') as f:
#     # data = json.load(f)
#     lines = f.readlines()

# for line in lines:
#     line = line.strip()
#     content = filter_msg(line)
#     if content == "":
#         continue
#     messages.append(content)

# âœ¦ å°è©±è½‰å‘é‡
embeddings = embedder.encode(messages, show_progress_bar=True, convert_to_tensor=True)
embeddings = np.array(embeddings.cpu())  # Convert to numpy array
# âœ¦ DBSCAN åˆ†ç¾¤åƒæ•¸
# eps: ç¯„åœåŠå¾‘ï¼ˆè¶Šå¤§ç¾¤è¶Šç²—ï¼‰
# min_samples: æ¯ç¾¤æœ€å°‘é»æ•¸ï¼ˆå»ºè­° 2~3ï¼‰
clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine')
labels = clustering.fit_predict(embeddings)

# âœ¦ æ•´ç†åˆ†ç¾¤çµæœ
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
clusters = [[] for _ in range(n_clusters)]
noise = []

for sentence, label in zip(messages, labels):
    if label == -1:
        noise.append(sentence)
    else:
        clusters[label].append(sentence)
        
print(f"åˆ†ç¾¤æ•¸é‡ï¼š{n_clusters}")
print(f"é›¢ç¾¤æ•¸é‡ï¼š{len(noise)}")
# âœ¦ é¡¯ç¤ºåˆ†ç¾¤çµæœ
for i, group in enumerate(clusters):
    print(f"\n--- ğŸ—‚ï¸ ä¸»é¡Œæ®µè½ {i+1} ---")
    print(f"å°è©±ï¼š{len(group)}")
    for line in group[:5]:
        print(f"  â¤ {line}")

    summary = " / ".join(group[:2]) + ("..." if len(group) > 2 else "")
    print(f"âœ¦ ç¸½çµï¼š{summary}")

    tokens = jieba.cut(" ".join(group), cut_all=False)
    tokens = list(set(tokens))  # Remove duplicates
    if tokens:
        keywords = kw_model.extract_keywords(" ".join(tokens), top_n=5)
        print(f"âœ¦ é—œéµå­—ï¼š{', '.join([k for k, _ in keywords])}")
    else:
        print("âœ¦ é—œéµå­—ï¼šç„¡æ³•æå–é—œéµå­—ï¼ˆç„¡æœ‰æ•ˆæ–‡æœ¬ï¼‰")
    # print(f"âœ¦ é—œéµå­—ï¼š{', '.join([k for k, _ in keywords])}")

# âœ¦ é¡¯ç¤ºé›¢ç¾¤é»ï¼ˆoptionalï¼‰
# if noise:
#     print(f"\n--- â— é›¢ç¾¤å¥ï¼ˆæœªæ­¸é¡ï¼‰ ---")
#     for sentence in noise:
#         print(f"  â¤ {sentence}")